{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Importing libraries.\n",
      "[INFO]: Defining custom classes and functions.\n",
      "[INFO]: Tokenizing data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 739/739 [00:00<00:00, 897.87 examples/s]\n",
      "Map: 100%|██████████| 159/159 [00:00<00:00, 868.30 examples/s]\n",
      "Map: 100%|██████████| 158/158 [00:00<00:00, 845.10 examples/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/work/Members' Files/DanielBlumenkranz#9679/NLP_exam_project/env/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "2023-12-13 11:07:19,514 - accelerate.utils.other - WARNING - Detected kernel version 5.4.256, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Training model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/Members' Files/DanielBlumenkranz#9679/NLP_exam_project/env/lib/python3.10/site-packages/intel_extension_for_pytorch/frontend.py:507: UserWarning: IPEX CPU does not support fused/fused split update for <class 'transformers.optimization.AdamW'> will use non-fused master weight update for bf16 training on CPU.\n",
      "  warnings.warn(\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='94' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/94 03:57 < 07:04, 0.14 it/s, Epoch 0.72/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"[INFO]: Importing libraries.\")\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, AutoConfig, \n",
    "    AutoTokenizer,\n",
    "    AdamW, get_linear_schedule_with_warmup,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback,\n",
    "    IntervalStrategy,\n",
    "    Trainer, TrainingArguments\n",
    ")\n",
    "\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "print(\"[INFO]: Defining custom classes and functions.\")\n",
    "\n",
    "# configuration\n",
    "CONFIG = {\n",
    "    \"model_name\": \"distilbert-base-uncased\",\n",
    "    \"device\": 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \"max_length\": 512,\n",
    "    \"train_batch_size\": 16, # dataset[train] is 739 long. That gives 93 batches/steps (last batch only has 3 data entries)\n",
    "    \"valid_batch_size\": 158, # 16 originally\n",
    "    \"epochs\": 2,\n",
    "    \"max_grad_norm\": 1000,\n",
    "    \"weight_decay\": 1e-6, # Btwn 0-0.1. \"The higher the value, the less likely your model will overfit. However, if set too high, your model might not be powerful enough.\"\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"loss_type\": \"rmse\",\n",
    "    \"n_accumulate\" : 1,\n",
    "    \"label_cols\" : ['Coherence', 'Empathy', 'Surprise', 'Engagement', 'Complexity'],\n",
    "    \"early_stopping_patience\": 2,\n",
    "    \"early_stopping_threshold\": 0.001,\n",
    "    \"seed\": 50 \n",
    "    \n",
    "}\n",
    "\n",
    "def tokenize(examples):\n",
    "    '''\n",
    "    A function to be used with the map method of the dataset class. \n",
    "    Tokenizes the text and returns a dictionary of tensors.\n",
    "    '''\n",
    "    labels = examples['label']\n",
    "    tokens = tokenizer(examples['text'], \n",
    "                       padding='max_length', \n",
    "                       truncation=True, \n",
    "                       max_length=CONFIG['max_length'], \n",
    "                       return_tensors='pt',\n",
    "                       return_attention_mask=True)\n",
    "    res = {\n",
    "        'input_ids': tokens['input_ids'].to(CONFIG.get('device')).squeeze(),\n",
    "        'attention_mask': tokens['attention_mask'].to(CONFIG.get('device')).squeeze(),\n",
    "        'labels': torch.tensor(labels)\n",
    "    }\n",
    "\n",
    "    return res\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    '''\n",
    "    A custom function that allows calculating the RMSE of each of the six metrics separately.\n",
    "    '''\n",
    "    predictions, labels = eval_pred\n",
    "    colwise_rmse = np.sqrt(np.mean((labels - predictions) ** 2, axis=0))\n",
    "    res = {\n",
    "        f\"{analytic.upper()}_RMSE\" : colwise_rmse[i]\n",
    "        for i, analytic in enumerate(CONFIG[\"label_cols\"])\n",
    "    }\n",
    "    res[\"MCRMSE\"] = np.mean(colwise_rmse)\n",
    "    return res\n",
    "\n",
    "class RegressionModel(nn.Module):\n",
    "    '''\n",
    "    A custom model that takes a pretrained model and adds a dropout layer and a linear layer on top.\n",
    "    '''\n",
    "    def __init__(self, model_name):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.config.hidden_dropout_prob = 0\n",
    "        self.config.attention_probs_dropout_prob = 0\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, config=self.config)\n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, len(CONFIG['label_cols']))\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.model(input_ids=input_ids, # out should be of type SequenceClassifierOutput\n",
    "                        attention_mask=attention_mask, \n",
    "                        output_hidden_states=True)\n",
    "        cls_token = out.hidden_states[-1][:, 0, :].to(CONFIG.get('device'))\n",
    "        out = self.drop(cls_token )\n",
    "        outputs = self.fc(out) # outputs should be regression scores\n",
    "        return SequenceClassifierOutput(logits=outputs)\n",
    "\n",
    "class RMSELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines the loss function to be fed into the CustomTrainer.\n",
    "    Code taken from Y Nakama's notebook (https://www.kaggle.com/code/yasufuminakama/fb3-deberta-v3-base-baseline-train)\n",
    "    \"\"\"\n",
    "    def __init__(self, eps=1e-9):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss(reduction='mean')\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        loss = torch.sqrt(self.mse(predictions, targets) + self.eps)\n",
    "        return loss\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    '''\n",
    "    A custom trainer class that overwrites the compute_loss method of Trainer to use RMSE loss\n",
    "    '''\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs = model(inputs['input_ids'], inputs['attention_mask']) # model outputs are of type SequenceClassifierOutput\n",
    "        loss_func = RMSELoss()\n",
    "        loss = loss_func(outputs.logits.float(), inputs['labels'].float()) # predictions, targets\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # paths\n",
    "    data_path = \"../story_eval_dataset_dict.pkl\"\n",
    "    models_path = \"../models/run2\"\n",
    "\n",
    "    # check if models_path exists, if not create it\n",
    "    #if not models_path.exists():\n",
    "    #    models_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # tokenize data\n",
    "    with open(data_path, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
    "\n",
    "    print('[INFO]: Tokenizing data.')\n",
    "    for split in dataset.keys():\n",
    "        dataset[split] = dataset[split].map(tokenize)\n",
    "\n",
    "    # calculate bactches per epoch\n",
    "    bacthes_per_epoch = math.ceil(len(dataset['train'])/(CONFIG['train_batch_size'] * CONFIG['n_accumulate']))\n",
    "\n",
    "    # define the training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=models_path,\n",
    "        evaluation_strategy=IntervalStrategy.STEPS,\n",
    "        save_strategy=IntervalStrategy.STEPS, # save checkpoint for each save_steps\n",
    "        eval_steps=bacthes_per_epoch, # compute metrics after each epoch\n",
    "        save_steps=bacthes_per_epoch,\n",
    "        logging_steps=bacthes_per_epoch,\n",
    "        logging_first_step=False,\n",
    "        logging_dir=models_path,\n",
    "        per_device_train_batch_size=CONFIG['train_batch_size'],\n",
    "        per_device_eval_batch_size=CONFIG['valid_batch_size'],\n",
    "        num_train_epochs=CONFIG['epochs'],\n",
    "        learning_rate=CONFIG['learning_rate'],\n",
    "        weight_decay=CONFIG['weight_decay'],\n",
    "        gradient_accumulation_steps=CONFIG['n_accumulate'],\n",
    "        use_cpu=True if CONFIG['device'] == 'cpu' else False,\n",
    "        use_ipex=True if CONFIG['device'] == 'cpu' else False,\n",
    "        bf16=True if CONFIG['device'] == 'cpu' else False,\n",
    "        seed=CONFIG['seed'],\n",
    "        group_by_length=True,\n",
    "        max_grad_norm=CONFIG['max_grad_norm'],\n",
    "        metric_for_best_model='eval_MCRMSE',\n",
    "        load_best_model_at_end=True, # always save best checkpoint at end of training. May exceed save_total_limit if best and last model are different.\n",
    "        greater_is_better=False,\n",
    "        save_total_limit=1,\n",
    "        label_names=[\"labels\"] \n",
    "    )\n",
    "\n",
    "    # data collator for dynamic padding\n",
    "    collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # define early stopping criteria\n",
    "    early_stop = EarlyStoppingCallback(early_stopping_patience = CONFIG['early_stopping_patience'], \n",
    "                                       early_stopping_threshold = CONFIG['early_stopping_threshold'])\n",
    "\n",
    "    # init model\n",
    "    model = RegressionModel(CONFIG['model_name'])\n",
    "    model.to(CONFIG['device'])\n",
    "\n",
    "    # count number of trainable params (total and in head)\n",
    "    #total = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    #modelhead = nn.Linear(model.config.hidden_size, len(CONFIG['label_cols']))\n",
    "    #head = sum(p.numel() for p in modelhead.parameters() if p.requires_grad)\n",
    "\n",
    "    # SET THE OPITMIZER AND THE SCHEDULER\n",
    "    # no decay for bias and normalization layers\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "    {\n",
    "            \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], # get all the params except those in no_decay\n",
    "            \"weight_decay\": CONFIG['weight_decay'],\n",
    "    },\n",
    "    {\n",
    "            \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], # get all the params that are in no_decay\n",
    "            \"weight_decay\": 0.0,\n",
    "    },\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_parameters, lr=CONFIG['learning_rate'])\n",
    "    \n",
    "    num_training_steps = bacthes_per_epoch * CONFIG['epochs']\n",
    "    #num_training_steps = (len(dataset['train']) * CONFIG['epochs']) // (CONFIG['train_batch_size'] * CONFIG['n_accumulate'])\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0.1*num_training_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    # init trainer\n",
    "    trainer = CustomTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset['train'],\n",
    "            eval_dataset=dataset['validation'],\n",
    "            data_collator=collate_fn,\n",
    "            optimizers=(optimizer, scheduler),\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[early_stop])\n",
    "    \n",
    "    # train\n",
    "    print(\"[INFO]: Training model.\")\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'loss': 1.1758,\n",
       "  'learning_rate': 1.1111111111111113e-05,\n",
       "  'epoch': 1.0,\n",
       "  'step': 47},\n",
       " {'eval_loss': 0.6899921298027039,\n",
       "  'eval_COHERENCE_RMSE': 0.7001960277557373,\n",
       "  'eval_EMPATHY_RMSE': 0.6809112429618835,\n",
       "  'eval_SURPRISE_RMSE': 0.6896708607673645,\n",
       "  'eval_ENGAGEMENT_RMSE': 0.6782934665679932,\n",
       "  'eval_COMPLEXITY_RMSE': 0.7005736827850342,\n",
       "  'eval_MCRMSE': 0.6899290680885315,\n",
       "  'eval_runtime': 19.4441,\n",
       "  'eval_samples_per_second': 8.126,\n",
       "  'eval_steps_per_second': 0.051,\n",
       "  'epoch': 1.0,\n",
       "  'step': 47},\n",
       " {'loss': 0.6679, 'learning_rate': 0.0, 'epoch': 2.0, 'step': 94},\n",
       " {'eval_loss': 0.6900824904441833,\n",
       "  'eval_COHERENCE_RMSE': 0.6998785138130188,\n",
       "  'eval_EMPATHY_RMSE': 0.6810686588287354,\n",
       "  'eval_SURPRISE_RMSE': 0.689216136932373,\n",
       "  'eval_ENGAGEMENT_RMSE': 0.6788548231124878,\n",
       "  'eval_COMPLEXITY_RMSE': 0.7010861039161682,\n",
       "  'eval_MCRMSE': 0.6900208592414856,\n",
       "  'eval_runtime': 19.1825,\n",
       "  'eval_samples_per_second': 8.237,\n",
       "  'eval_steps_per_second': 0.052,\n",
       "  'epoch': 2.0,\n",
       "  'step': 94},\n",
       " {'train_runtime': 703.6932,\n",
       "  'train_samples_per_second': 2.1,\n",
       "  'train_steps_per_second': 0.134,\n",
       "  'total_flos': 0.0,\n",
       "  'train_loss': 0.9218511378511469,\n",
       "  'epoch': 2.0,\n",
       "  'step': 94}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.state.log_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'train_runtime': 703.6932,\n",
       "  'train_samples_per_second': 2.1,\n",
       "  'train_steps_per_second': 0.134,\n",
       "  'total_flos': 0.0,\n",
       "  'train_loss': 0.9218511378511469,\n",
       "  'epoch': 2.0,\n",
       "  'step': 94},\n",
       " [{'Training Loss': 1.1758,\n",
       "   'Epoch': 1.0,\n",
       "   'Step': 47,\n",
       "   'Validation Loss': 0.6899921298027039,\n",
       "   'Coherence Rmse': 0.7001960277557373,\n",
       "   'Empathy Rmse': 0.6809112429618835,\n",
       "   'Surprise Rmse': 0.6896708607673645,\n",
       "   'Engagement Rmse': 0.6782934665679932,\n",
       "   'Complexity Rmse': 0.7005736827850342,\n",
       "   'Mcrmse': 0.6899290680885315},\n",
       "  {'Training Loss': 0.6679,\n",
       "   'Epoch': 2.0,\n",
       "   'Step': 94,\n",
       "   'Validation Loss': 0.6900824904441833,\n",
       "   'Coherence Rmse': 0.6998785138130188,\n",
       "   'Empathy Rmse': 0.6810686588287354,\n",
       "   'Surprise Rmse': 0.689216136932373,\n",
       "   'Engagement Rmse': 0.6788548231124878,\n",
       "   'Complexity Rmse': 0.7010861039161682,\n",
       "   'Mcrmse': 0.6900208592414856}],\n",
       " {'Loss': 0.6900824904441833,\n",
       "  'Coherence Rmse': 0.6998785138130188,\n",
       "  'Empathy Rmse': 0.6810686588287354,\n",
       "  'Surprise Rmse': 0.689216136932373,\n",
       "  'Engagement Rmse': 0.6788548231124878,\n",
       "  'Complexity Rmse': 0.7010861039161682,\n",
       "  'Mcrmse': 0.6900208592414856})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.modelcard import parse_log_history\n",
    "log_history = parse_log_history(trainer.state.log_history)\n",
    "log_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../models/run3/log_history\", 'rb') as f:\n",
    "    log_history = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Training Loss': 0.8029,\n",
       " 'Epoch': 10.0,\n",
       " 'Step': 240,\n",
       " 'Validation Loss': 0.8540864586830139,\n",
       " 'Coherence Rmse': 0.9171026349067688,\n",
       " 'Empathy Rmse': 0.8103721141815186,\n",
       " 'Surprise Rmse': 0.7883448004722595,\n",
       " 'Engagement Rmse': 0.8322986364364624,\n",
       " 'Complexity Rmse': 0.9139639735221863,\n",
       " 'Mcrmse': 0.8524163961410522}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_history[1][-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
